\paragraph{}
Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are two methods used to
reduce the dimension of a data-set. They are very useful as a pre-processing step, which has proven
to greatly improve results of further data analysis. The task consists of removing noise,
de-correlating and reducing the dimension of the data. This step enables a better insight into
the true latent structure and the hidden patterns of the data-set. These methods are widely used in a
variety of applications where data-analysis is involved such as data-mining, classification, text
analysis and computer vision.

In sections \ref{sec.pca} and \ref{sec.lda} we present the basic version of PCA and LDA. In section
\ref{sec.kerneltrick}, we outline their limitations and introduce the famous \emph{Kernel Trick} to
overcome them. Section \ref{sec.kpca} and \ref{sec.klda} describe the equations of the kernel trick
applied to the PCA and LDA.
