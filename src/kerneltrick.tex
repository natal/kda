\paragraph{}
The two previous standard methods assume that the data is linearly correlated in feature space. This
is a deep assumption that rarely matches real-life data-sets. When it comes to real applications, we
are interested in finding complex hidden patterns that are caracterised by non-linear properties.

\paragraph{}
In order to gain access to these non-linear hidden patterns, the idea is to map the data vectors into
a high-dimensional space known as \emph{kernel-space}. The mapping function $\varphi$ carries out
this task. We can then substitute every $x$ vector by its corresponding $\varphi(x)$ in the PCA and
LDA equations.

\paragraph{}
However, the computation of $\varphi(x)$ usually turns out to be intractable because the
\emph{kernel-space} is allowed to have a dimension equal to infinity. Moreover, in some occurences,
the $\varphi$ mapping function is unknown in the sense that the only information we have is the
result of the dot product in kernel space but not the function itself.

\paragraph{}
To avoid this difficulty, we use what is known as the \emph{kernel trick}. Instead of computing
$\varphi(x)$, we use a \emph{kernel function} $k(x, y) = \varphi(x)^t\varphi(y)$ that computes
the dot product of $x$ and $y$ in \emph{kernel-space} so that we never have to compute the result of
$\varphi(x)$ directly. In order to use this trick, we must adapt the PCA and LDA formulas, keeping in
mind that we can only use kernel-space dot products.
