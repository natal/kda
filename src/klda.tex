\paragraph{}
The kernel trick can also be applied to the LDA.

\paragraph{}
The key to adapting the LDA is to notice that an eigenvector $w$ in kernel space can be written as a
linear combination of all the data points in kernel space.

\begin{equation}
\label{phibase}
w = \sum_{i=1}^N \alpha_i \varphi(x_i)
\end{equation}

\paragraph{}
From there, we can move on with rewritting the quotient $J = \frac{w^tS_bw}{w^tS_ww}$ which we are
trying to maximize.

\paragraph{}
We start with the between-class covarience matrix:

\begin{equation*}
w^tS_bw = \left( \sum_{l=1}^N \alpha_l \varphi(x_l)\right)^t
            \left( \sum_{c=1}^C N_c(\lambda_c - \lambda)(\lambda_c - \lambda)^t\right)
            \left( \sum_{n=1}^N \alpha_n \varphi(x_n)\right)^t 
\end{equation*}

The latter can be rearranged into:

\begin{equation*}
w^tS_bw = \sum_{l=1}^N\alpha_l \sum_{n=1}^N \sum_{c =1}^C 
                \varphi(x_l)^t(\lambda_c - \lambda)(\lambda_c - \lambda)^t\varphi(x_n)
\end{equation*}

This can be viewed as the folowing quadratic expression:

\begin{equation*}
w^tS_bw = \alpha^tM\alpha
\end{equation*}

Where,

\begin{equation*}
(M)_{l,n} = \sum_{c=1}^C N_c ~\varphi(x_l)^t (\lambda_c - \lambda)(\lambda_c -\lambda)^t
\varphi(x_n)
\end{equation*}

\begin{equation*}
\alpha = \left( \begin{array}{c}
                                \alpha_1 \\
                                ... \\
                                \alpha_N
                  \end{array}  \right) 
\end{equation*}

Furthermore, we can notice that the $M$ matrix can be broken down into a product of two matrices: $ M = M_1M_2$, with:

\begin{align*}
(M_1)_{l, c} &= \varphi(x_l)^t(\lambda_c - \lambda)\\
             &= (\varphi(x_l)^t\lambda_c -\varphi(x_l)^t\lambda) \\
             &= \left( \frac{1}{N_c} \sum_{i=1}^{N_c} \varphi(x_l)^t\varphi(x_i^c)\right) -
                 \frac{1}{N}\sum_{i=1}^N\phi(x_l)^t\phi(x_i) \\
             &= \frac{1}{N_c} \left(\sum_{i = 1}^{N_c} k(x_l, x_i^c) -
                \frac{1}{N}\sum_{i=1}^N k(x_l, x_i) \right)\\
\end{align*}

In addition, we can also notice that: $M_2 = M_1^t$.

\paragraph{}
A more convenient way of writting $M_1$ is by considering the matrix $\mathbb{I} \in
\mathcal{M}_{N,C}$ defined by:

\begin{equation*}
(\mathbb{I})_{i, c} = \left\{ \begin{array}{ll}
                                  1 & \text{if $x_i$ belongs to class $c$} \\
                                  0 & \text{otherwise}
                               \end{array}
                        \right.
\end{equation*}

And by considering the matrix $1 \in \mathcal{M}_{N,C}$ that is exclusively composed of
1's.
We can rewrite the $M$ matrix as:

\begin{equation*}
M = G\left( \mathbb{I} - \frac{1}{N} 1 \right)\left( \mathbb{I} - \frac{1}{N} 1\right)^tG^t
\end{equation*}

\paragraph{}
We follow by expressing the within-class covariance in kernel space:

\begin{equation*}
S_w = \sum_{c = 1}^C \frac{1}{N_c} \sum_{i = 1}^{N_c} (\varphi(x_i^c) - \lambda_c)(\varphi(x_i^c) -
\lambda_c)^t
\end{equation*}

Using to ? we can rewrite $w^tS_ww$ by substituting the $w$ vectors in a similar fashion as
with $S_b$.

\begin{align*}
  w^tS_ww &= \left( \sum_{l=1}^N \alpha_l \varphi(x_l)\right)^t
             \left( \sum_{c=1}^C \frac{1}{C}
                       \sum_{i=1}^{N_c} (\varphi(x_i^c) - \lambda_c)
                                        (\varphi(x_i^c) - \lambda_c)^t\right)
             \left( \sum_{n=1}^N \alpha_n \varphi(x_n)\right)\\
          &= \sum_{l=1}^N \alpha_l \sum_{n=1}^N \alpha_n \sum_{c=1}^C \frac{1}{C}
                \sum_{i=1}^{N_c} (\varphi(x_i^c) - \lambda_c)
                                 (\varphi(x_i^c) - \lambda_c)^t
\end{align*}

Again, we notice that the previous result is a quadratic expression which we will write as:

\begin{equation*}
w^tS_ww = \alpha^tN\alpha
\end{equation*}

The goal is to identify the content of matrix $N$:

\begin{align*}
  (N)_{n,l} &= \sum_{c=1}^C \frac{1}{N_c} \sum_{i=1}^{N_c}
                  \varphi(x_l)^t(\varphi(x_i^c) - \lambda_c)\varphi(x_n)^t \\
            &= \sum_{c=1}^C \frac{1}{N_c} \sum_{i=1}^{N_c} (\varphi(x_l)^t\varphi(x_i^c) -
            \varphi(x_l)^t\lambda_c) (\varphi(x_i^c)\varphi(x_n)^t -
            \lambda_c \varphi(x_n)^t)
\end{align*}

From there, we need to express matrix $N$ according to the Gramm matrix. We start by noticing that,
$N$ is a sum of the products between two different matrices:

\begin{equation*}
N = \sum_{c=1}^C \frac{1}{N_c} N^c (N^c)^t
\end{equation*}

Where,

\begin{align*}
  N^c_{l, i} &= \varphi(x_l)^t\varphi(x_i^c) - \varphi(x_l)^t \frac{1}{N_c}\sum_{j=1}^{N_c}\varphi(x_j^c) \\
             &= k(x_l, x_i^c) - \frac{1}{N_c}\sum_{j=1}^{N_c}k(x_l, x_j^c) \\
             &= G^{X, X_c} - \frac{1}{N_c} G^{X, X_c} 1_{(N_c, N)} \\
             &= G^{X, X_c} (I - \frac{1}{N_c}1_{(N_c, N)})
\end{align*}

With $G^{X, X_c}$ being the Gramm matrix of data set $X$ against data set $X_c$.

\paragraph{}
The original maximization problem \ref{maximize} is reformulated as follows:

\begin{equation*}
\label{maximize2}
\boxed{\operatorname*{arg\,max}_\alpha \frac{\alpha^t M \alpha}{\alpha^t N \alpha}}
\end{equation*}

Since $\alpha$ is an eigen vector 





