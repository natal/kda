\paragraph{}
The kernel trick can also be applied to the LDA. Let's write the equations of $S_b$ and $S_w$ in kernel-space:

\begin{align*}
S_b^\varphi &= \sum_{c = 1}^C N_c(\lambda_c - \lambda)(\lambda_c - \lambda)^t \\
S_w^\varphi &= \sum_{c = 1}^{C}\sum_{x_i^c \in c} (x_i^c - \lambda_c)(x_i^c - \lambda_c)^t
\end{align*}

\paragraph{}
The key to adapting the LDA is to notice that an eigenvector $w$ in kernel space can be written as a
linear combination of all the data points in kernel space.

\begin{equation}
\label{eq:phibase}
w = \sum_{i=1}^N \alpha_i \varphi(x_i)
\end{equation}

\paragraph{}
From there, we can move on with rewritting the quotient $J = \frac{w^tS_b^\varphi w}{w^tS_w^\varphi w}$ which we are
trying to maximize.

\paragraph{}
We start with the between-class covarience matrix:

\begin{equation*}
w^tS_b^\varphi w = \left( \sum_{l=1}^N \alpha_l \varphi(x_l)\right)^t
            \left( \sum_{c=1}^C N_c(\lambda_c - \lambda)(\lambda_c - \lambda)^t\right)
            \left( \sum_{n=1}^N \alpha_n \varphi(x_n)\right)
\end{equation*}

The latter can be rearranged into:

\begin{equation*}
w^tS_bw = \sum_{l=1}^N\alpha_l \sum_{n=1}^N \alpha_n \sum_{c =1}^C
                N_c ~\varphi(x_l)^t(\lambda_c - \lambda)(\lambda_c - \lambda)^t\varphi(x_n)
\end{equation*}

This can be viewed as the folowing quadratic expression:

\begin{equation*}
w^tS_bw = \alpha^tM\alpha
\end{equation*}

Where,

\begin{equation*}
(M)_{l,n} = \sum_{c=1}^C N_c ~\varphi(x_l)^t (\lambda_c - \lambda)(\lambda_c -\lambda)^t
\varphi(x_n)
\end{equation*}

\begin{equation*}
\alpha = \left( \begin{array}{c}
                                \alpha_1 \\
                                ... \\
                                \alpha_N
                  \end{array}  \right) 
\end{equation*}

Furthermore, we can notice that the $M$ matrix can be broken down into a product of two matrices: $M = M_1M_2$, with:

\begin{align*}
(M_1)_{l, c} &= \sqrt{N_c}~\varphi(x_l)^t(\lambda_c - \lambda)\\
             &= \sqrt{N_c}\left(\varphi(x_l)^t\lambda_c -\varphi(x_l)^t\lambda\right) \\
             &= \sqrt{N_c}\left(\frac{1}{N_c} \sum_{i=1}^{N_c} \varphi(x_l)^t\varphi(x_i^c) -
                 \frac{1}{N}\sum_{i=1}^N\varphi(x_l)^t\varphi(x_i)\right) \\
             &= \sqrt{N_c}\left(\frac{1}{N_c} \sum_{i = 1}^{N_c} k(x_l, x_i^c) -
                \frac{1}{N}\sum_{i=1}^N k(x_l, x_i)\right) \\
\end{align*}

In addition, we can also notice that: $M_2 = M_1^t$.

\paragraph{}
A more convenient way of writting $M_1$ is by considering the matrix $\mathds{I} \in
\mathcal{M}_{N,C}$ defined by:

$$ (\mathds{I})_{i, c} = \left\{ \begin{array}{ll}
                                  \frac{1}{\sqrt{N_c}} & \textnormal{if $x_i$ belongs to class $c$} \\
                                  0 & \textnormal{otherwise}
                               \end{array}
                        \right.$$
Also by considering the matrix $\mathds{1} \in \mathcal{M}_{N,C}$ that is defined by:
$$ (\mathds{1})_{i, c} = \sqrt{N_c} $$
We can rewrite the $M_1$ matrix as:

$$M_1 = G\left( \mathds{I} - \frac{1}{N}\mathds{1} \right)$$

And then $M$ as:

$$M = G\left( \mathds{I} - \frac{1}{N} \mathds{1} \right)\left( \mathds{I} - \frac{1}{N}
\mathds{1}\right)^tG^t$$

\paragraph{}
We follow by expressing the within-class covariance in kernel space:

\begin{equation*}
S_w = \sum_{c = 1}^C \frac{1}{N_c} \sum_{i = 1}^{N_c} (\varphi(x_i^c) - \lambda_c)(\varphi(x_i^c) -
\lambda_c)^t
\end{equation*}

Using \ref{eq:phibase} we can rewrite $w^tS_ww$ by substituting the $w$ vectors in a similar fashion
to how $w^tS_bw$ was adapted.

\begin{align*}
  w^tS_ww &= \left( \sum_{l=1}^N \alpha_l \varphi(x_l)\right)^t
             \left( \sum_{c=1}^C \frac{1}{C}
                       \sum_{i=1}^{N_c} (\varphi(x_i^c) - \lambda_c)
                                        (\varphi(x_i^c) - \lambda_c)^t\right)
             \left( \sum_{n=1}^N \alpha_n \varphi(x_n)\right)\\
          &= \sum_{l=1}^N \alpha_l \sum_{n=1}^N \alpha_n \sum_{c=1}^C \frac{1}{C}
                \sum_{i=1}^{N_c} (\varphi(x_i^c) - \lambda_c)
                                 (\varphi(x_i^c) - \lambda_c)^t
\end{align*}

Again, we notice that the previous result is a quadratic expression which we can write as:

\begin{equation*}
w^tS_ww = \alpha^tW\alpha
\end{equation*}

The goal is to identify the content of matrix $W$:

\begin{align*}
  (W_c)_{n,l} &= \sum_{c=1}^C \frac{1}{N_c} \sum_{i=1}^{N_c}
                  \varphi(x_l)^t(\varphi(x_i^c) - \lambda_c)\varphi(x_n)^t \\
            &= \sum_{c=1}^C \frac{1}{N_c} \sum_{i=1}^{N_c} (\varphi(x_l)^t\varphi(x_i^c) -
            \varphi(x_l)^t\lambda_c) (\varphi(x_i^c)\varphi(x_n)^t -
            \lambda_c \varphi(x_n)^t)
\end{align*}

From there, we need to express matrix $N$ according to the Gramm matrix. We start by noticing that,
$N$ is a sum of the products between two matrices:

\begin{equation*}
W = \sum_{c=1}^C \frac{1}{N_c} W_c (W_c)^t
\end{equation*}

Where,

\begin{align*}
  (W_c)_{l, i} &= \varphi(x_l)^t\varphi(x_i^c) - \varphi(x_l)^t \frac{1}{N_c}\sum_{j=1}^{N_c}\varphi(x_j^c) \\
             &= k(x_l, x_i^c) - \frac{1}{N_c}\sum_{j=1}^{N_c}k(x_l, x_j^c) \\
   W_c       &= G_{X, X_c} - \frac{1}{N_c} G_{X, X_c} \mathds{1}_{(N_c, N)} \\
   W_c       &= G_{X, X_c} \left(\textrm{I} - \frac{1}{N_c}\mathds{1}_{(N_c, N)}\right)
\end{align*}

With $X_c$ being the data set containing only the vectors of class $x$, $\textrm{I}$ the identity matrix of
size $N_c \times N$ and
$G_{X, X_c}$ being the Gramm matrix of data set $X$ against data set $X_c$.

\paragraph{}
The original maximization problem \ref{eq:maximize} is reformulated as follows:

\begin{equation*}
\label{eq:maximize2}
\boxed{\operatorname*{arg\,max}_\alpha \frac{\alpha^t M \alpha}{\alpha^t W \alpha}}
\end{equation*}

Recalling \ref{eq:phibase}, we can infer the following allowing us to project the set $X$ to the
lower dimensional set $X_p$:

\begin{align*}
  w &= \sum_{i=1}^N \alpha_i \varphi(x_i) = \\
  w^t \varphi(x_j) &= \sum_{i = 1}^N \alpha_i \varphi(x_i)^t \varphi(x_j)\\
                   &= \sum_{i = 1}^N \alpha_i k(x_i, x_j) \\
\end{align*}

We know that $\varphi(x)$, with $x \in Y$, can be projected to $x_p$ through $x_p = w^t \varphi(x)$.
Given the latter identity, we can write:
\begin{align*}
x_p &= \sum_{i = 1}^N \alpha_i k(x_i, x_p) \\
X_p &= \alpha^t G_{X,Y} \\
\end{align*}
