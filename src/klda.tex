\paragraph{}
The kernel trick can also be applied to the LDA. Let's write the equations of $S_b$ and $S_w$ in kernel-space:

\begin{align*}
S_b^\varphi &= \sum_{c = 1}^C N_c(\lambda_c - \lambda)(\lambda_c - \lambda)^t \\
S_w^\varphi &= \sum_{c = 1}^{C}\sum_{i = 1}^{N_c} (x_i^c - \lambda_c)(x_i^c - \lambda_c)^t
\end{align*}

\paragraph{}
The key to adapting the LDA is to notice that an eigenvector $w$ in kernel-space can be written as a
linear combination of all the data vectors in kernel-space:

\begin{equation}
\label{eq:phibase}
w = \sum_{i=1}^N \alpha_i \varphi(x_i)
\end{equation}

\paragraph{}
From there, we can move on with rewriting the quotient $J = \frac{w^tS_b^\varphi w}{w^tS_w^\varphi w}$ which we are
trying to maximize. We start with the between-class covarience matrix:

\begin{align*}
w^tS_b^\varphi w &= \left( \sum_{l=1}^N \alpha_l \varphi(x_l)\right)^t
            \left( \sum_{c=1}^C N_c(\lambda_c - \lambda)(\lambda_c - \lambda)^t\right)
            \left( \sum_{n=1}^N \alpha_n \varphi(x_n)\right) \\
&= \sum_{l=1}^N\alpha_l \sum_{n=1}^N \alpha_n \sum_{c =1}^C
                N_c ~\varphi(x_l)^t(\lambda_c - \lambda)(\lambda_c - \lambda)^t\varphi(x_n) \\
                &= \alpha^tM\alpha
\end{align*}

Where,

\begin{align*}
(M)_{l,n} &= \sum_{c=1}^C N_c ~\varphi(x_l)^t (\lambda_c - \lambda)(\lambda_c -\lambda)^t \varphi(x_n) \\
\alpha &= \left( \begin{array}{c}
                                \alpha_1 \\
                                \vdots \\
                                \alpha_N
                  \end{array}  \right) 
\end{align*}

Furthermore, we can notice that the $M$ matrix can be broken down into a product of two matrices: $M_1M_2$, where $M_1 = M_2^t$, with:

\begin{align*}
(M_1)_{l, c} &= \sqrt{N_c}~\varphi(x_l)^t(\lambda_c - \lambda)\\
             &= \sqrt{N_c}\left(\varphi(x_l)^t\lambda_c -\varphi(x_l)^t\lambda\right) \\
             &= \sqrt{N_c}\left(\frac{1}{N_c} \sum_{i=1}^{N_c} \varphi(x_l)^t\varphi(x_i^c) -
                 \frac{1}{N}\sum_{i=1}^N\varphi(x_l)^t\varphi(x_i)\right) \\
             &= \sqrt{N_c}\left(\frac{1}{N_c} \sum_{i = 1}^{N_c} k(x_l, x_i^c) -
                \frac{1}{N}\sum_{i=1}^N k(x_l, x_i)\right) \\
\end{align*}

\paragraph{}
A more convenient way of writing $M_1$ is by considering the matrix $\mathds{I} \in \mathcal{M}_{N,C}$ defined by:

$$ (\mathds{I})_{i, c} = \left\{ \begin{array}{ll}
                                  \frac{1}{\sqrt{N_c}} & \textnormal{if $x_i$ belongs to class $c$} \\
                                  0 & \textnormal{otherwise}
                               \end{array}
                        \right.$$
Also by considering the matrix $\mathds{1} \in \mathcal{M}_{N,C}$ that is defined by:
$$ (\mathds{1})_{i, c} = \sqrt{N_c} $$
We can rewrite the $M_1$ matrix as:

$$M_1 = G\left( \mathds{I} - \frac{\mathds{1}}{N} \right)$$

And then $M$ as:

$$M = G\left( \mathds{I} - \frac{\mathds{1}}{N}\right)\left(\mathds{I} - \frac{\mathds{1}}{N}\right)^tG^t$$

\paragraph{}
We follow by expressing the within-class covariance in kernel space:

\begin{equation*}
S_w = \sum_{c = 1}^C \sum_{i = 1}^{N_c} (\varphi(x_i^c) - \lambda_c)(\varphi(x_i^c) -
\lambda_c)^t
\end{equation*}

Using equation (\ref{eq:phibase}) we can rewrite $w^tS_ww$ by substituting the $w$ vectors in a similar fashion
to how $w^tS_bw$ was adapted:

\begin{align*}
  w^tS_ww &= \left( \sum_{l=1}^N \alpha_l \varphi(x_l)\right)^t
             \left( \sum_{c=1}^C \sum_{i=1}^{N_c} (\varphi(x_i^c) - \lambda_c) (\varphi(x_i^c) - \lambda_c)^t\right) \left( \sum_{n=1}^N \alpha_n \varphi(x_n)\right) \\
          &= \sum_{l=1}^N \alpha_l \sum_{n=1}^N \alpha_n \sum_{c=1}^C
          \sum_{i=1}^{N_c} \varphi(x_l)^t(\varphi(x_i^c) - \lambda_c)
                                 (\varphi(x_i^c) - \lambda_c)^t  \varphi(x_m)^t\\
&= \alpha^tW\alpha
\end{align*}


The goal is to identify the content of matrix $W$:

\begin{align*}
  (W_c)_{n,l} &= \sum_{c=1}^C \sum_{i=1}^{N_c}
                  \varphi(x_l)^t(\varphi(x_i^c) - \lambda_c)(\varphi(x_i^c) - \lambda_c)^t\varphi(x_n)^t \\
            &= \sum_{c=1}^C \sum_{i=1}^{N_c} (\varphi(x_l)^t\varphi(x_i^c) -
            \varphi(x_l)^t\lambda_c) (\varphi(x_i^c)\varphi(x_n)^t -
            \lambda_c \varphi(x_n)^t)
\end{align*}

From there, we need to express matrix $W$ according to the Gramm matrix. We start by noticing that,
$W$ is a sum of the products between two matrices:

\begin{equation*}
W = \sum_{c=1}^C W_c (W_c)^t
\end{equation*}

Where,

\begin{align*}
  (W_c)_{l, i} &= \varphi(x_l)^t\varphi(x_i^c) - \varphi(x_l)^t \frac{1}{N_c}\sum_{j=1}^{N_c}\varphi(x_j^c) \\
             &= k(x_l, x_i^c) - \frac{1}{N_c}\sum_{j=1}^{N_c}k(x_l, x_j^c) \\
   W_c       &= G_{X, X_c} - \frac{1}{N_c} G_{X, X_c} \mathds{1}_{(N_c, N)} \\
   W_c       &= G_{X, X_c} \left(\textrm{I} - \frac{1}{N_c}\mathds{1}_{(N_c, N)}\right)
\end{align*}

With $X_c$ being the data set containing only the vectors of class $c$, $\textrm{I}$ the identity matrix of
size $N_c \times N$ and
$G_{X, X_c}$ being the Gramm matrix of data set $X$ against data-set $X_c$.

\paragraph{}
The original maximization problem (\ref{eq:maximize}) is reformulated as follows:

\begin{equation*}
\label{eq:maximize2}
\operatorname*{arg\,max}_\alpha \frac{\alpha^t M \alpha}{\alpha^t W \alpha}
\end{equation*}

Recalling equation (\ref{eq:phibase}), we can infer the following allowing us to project the set $X$ to the
lower dimensional set $X_p$:

\begin{align*}
  w &= \sum_{i=1}^N \alpha_i \varphi(x_i) \\
  w^t \varphi(x_j) &= \sum_{i = 1}^N \alpha_i \varphi(x_i)^t \varphi(x_j)\\
                   &= \sum_{i = 1}^N \alpha_i k(x_i, x_j) \\
\end{align*}

We know that $\varphi(x)$, with $x \in Y$, can be projected to $x_p$ through $x_p = w^t \varphi(x)$.
Given the latter identity, we can write:
\begin{align*}
x_p &= \sum_{i = 1}^N \alpha_i k(x_i, x_p) \\
X_p &= \alpha^t G_{X,Y} \\
\end{align*}
