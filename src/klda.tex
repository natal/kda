\paragraph{}
The kernel trick can also be applied to the LDA.

\paragraph{}
The key to adapting the LDA is to notice that an eigenvector $w$ in kernel space can be written as a
linear combination of all the data points in kernel space.

\begin{equation}
\label{eq:phibase}
w = \sum_{i=1}^N \alpha_i \varphi(x_i)
\end{equation}

\paragraph{}
From there, we can move on with rewritting the quotient $J = \frac{w^tS_bw}{w^tS_ww}$ which we are
trying to maximize.

\paragraph{}
We start with the between-class covarience matrix:

\begin{equation*}
w^tS_bw = \left( \sum_{l=1}^N \alpha_l \varphi(x_l)\right)^t
            \left( \sum_{c=1}^C N_c(\lambda_c - \lambda)(\lambda_c - \lambda)^t\right)
            \left( \sum_{n=1}^N \alpha_n \varphi(x_n)\right)^t 
\end{equation*}

The latter can be rearranged into:

\begin{equation*}
w^tS_bw = \sum_{l=1}^N\alpha_l \sum_{n=1}^N \sum_{c =1}^C
                \varphi(x_l)^t(\lambda_c - \lambda)(\lambda_c - \lambda)^t\varphi(x_n)
\end{equation*}

This can be viewed as the folowing quadratic expression:

\begin{equation*}
w^tS_bw = \alpha^tB\alpha
\end{equation*}

Where,

\begin{equation*}
(B)_{l,n} = \sum_{c=1}^C N_c ~\varphi(x_l)^t (\lambda_c - \lambda)(\lambda_c -\lambda)^t
\varphi(x_n)
\end{equation*}

\begin{equation*}
\alpha = \left( \begin{array}{c}
                                \alpha_1 \\
                                ... \\
                                \alpha_N
                  \end{array}  \right) 
\end{equation*}

Furthermore, we can notice that the $M$ matrix can be broken down into a product of two matrices: $B = B_1B_2$, with:

\begin{align*}
(B_1)_{l, c} &= \varphi(x_l)^t(\lambda_c - \lambda)\\
             &= (\varphi(x_l)^t\lambda_c -\varphi(x_l)^t\lambda) \\
             &= \left( \frac{1}{N_c} \sum_{i=1}^{N_c} \varphi(x_l)^t\varphi(x_i^c)\right) -
                 \frac{1}{N}\sum_{i=1}^N\phi(x_l)^t\phi(x_i) \\
             &= \frac{1}{N_c} \left(\sum_{i = 1}^{N_c} k(x_l, x_i^c) -
                \frac{1}{N}\sum_{i=1}^N k(x_l, x_i) \right)\\
\end{align*}

In addition, we can also notice that: $M_2 = M_1^t$.

\paragraph{}
A more convenient way of writting $B_1$ is by considering the matrix $\mathds{I} \in
\mathcal{B}_{N,C}$ defined by:

$$ (\mathds{I})_{i, c} = \left\{ \begin{array}{ll}
                                  1 & \textnormal{if $x_i$ belongs to class $c$} \\
                                  0 & \textnormal{otherwise}
                               \end{array}
                        \right.$$
Also by considering the matrix $\mathds{1} \in \mathcal{B}_{N,C}$ that is exclusively composed of
1's.
We can rewrite the $M$ matrix as:

$$B = G\left( \mathds{I} - \frac{1}{N} \mathds{1} \right)\left( \mathds{I} - \frac{1}{N} \mathds{1}\right)^tG^t$$

\paragraph{}
We follow by expressing the within-class covariance in kernel space:

\begin{equation*}
S_w = \sum_{c = 1}^C \frac{1}{N_c} \sum_{i = 1}^{N_c} (\varphi(x_i^c) - \lambda_c)(\varphi(x_i^c) -
\lambda_c)^t
\end{equation*}

Using \ref{eq:phibase} we can rewrite $w^tS_ww$ by substituting the $w$ vectors in a similar fashion as
with $S_b$.

\begin{align*}
  w^tS_ww &= \left( \sum_{l=1}^N \alpha_l \varphi(x_l)\right)^t
             \left( \sum_{c=1}^C \frac{1}{C}
                       \sum_{i=1}^{N_c} (\varphi(x_i^c) - \lambda_c)
                                        (\varphi(x_i^c) - \lambda_c)^t\right)
             \left( \sum_{n=1}^N \alpha_n \varphi(x_n)\right)\\
          &= \sum_{l=1}^N \alpha_l \sum_{n=1}^N \alpha_n \sum_{c=1}^C \frac{1}{C}
                \sum_{i=1}^{N_c} (\varphi(x_i^c) - \lambda_c)
                                 (\varphi(x_i^c) - \lambda_c)^t
\end{align*}

Again, we notice that the previous result is a quadratic expression which we will write as:

\begin{equation*}
w^tS_ww = \alpha^tW\alpha
\end{equation*}

The goal is to identify the content of matrix $W$:

\begin{align*}
  (W)_{n,l} &= \sum_{c=1}^C \frac{1}{N_c} \sum_{i=1}^{N_c}
                  \varphi(x_l)^t(\varphi(x_i^c) - \lambda_c)\varphi(x_n)^t \\
            &= \sum_{c=1}^C \frac{1}{N_c} \sum_{i=1}^{N_c} (\varphi(x_l)^t\varphi(x_i^c) -
            \varphi(x_l)^t\lambda_c) (\varphi(x_i^c)\varphi(x_n)^t -
            \lambda_c \varphi(x_n)^t)
\end{align*}

From there, we need to express matrix $N$ according to the Gramm matrix. We start by noticing that,
$N$ is a sum of the products between two different matrices:

\begin{equation*}
W = \sum_{c=1}^C \frac{1}{N_c} W_c (N^c)^t
\end{equation*}

Where,

\begin{align*}
  (W_c)_{l, i} &= \varphi(x_l)^t\varphi(x_i^c) - \varphi(x_l)^t \frac{1}{N_c}\sum_{j=1}^{N_c}\varphi(x_j^c) \\
             &= k(x_l, x_i^c) - \frac{1}{N_c}\sum_{j=1}^{N_c}k(x_l, x_j^c) \\
   W_c       &= G^{X, X_c} - \frac{1}{N_c} G^{X, X_c} \mathds{1}_{(N_c, N_c)} \\
   W_c       &= G^{X, X_c} \left(\textrm{I} - \frac{1}{N_c}\mathds{1}_{(N_c, N_c)}\right)
\end{align*}

With $X_c$ being the data set containing only the vectors of class $x$, $\textrm{I}$ the identity matrix of
size $N_c \times N_c$ and
$G^{X, X_c}$ being the Gramm matrix of data set $X$ against data set $X_c$.

\paragraph{}
The original maximization problem \ref{eq:maximize} is reformulated as follows:

\begin{equation*}
\label{eq:maximize2}
\boxed{\operatorname*{arg\,max}_\alpha \frac{\alpha^t B \alpha}{\alpha^t W \alpha}}
\end{equation*}

Recalling \ref{eq:phibase}, we can infer the following:

\begin{align*}
  w &= \sum_{i=1}^N \alpha_i \varphi(x_i)\\
  w^t \varphi(x_j) &= \sum_{i = 1}^N \alpha_i \varphi(x_i)^t \varphi(x_j)\\
                   &= \sum_{i = 1}^N \alpha_i k(x_i, x_j) \\
  w^t \varphi(X) &=  \alpha^t G \\
\end{align*}





