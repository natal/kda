\paragraph{}
Regarding the PCA, we must first center the data vectors in kernel-space by subtracting the global mean in kernel-space $\mu$ to each one.
Therefore, the first step is to compute $\mu$:

$$\mu = \frac{1}{N}\sum_{i = 1}^{N} \varphi(x_i)$$

This formula is intractable for the reasons mentioned previously.  In addition,
let $\Phi$ be the matrix that contains all the dataset vectors centered in
kernel-space. The $i$th column of matrix $\Phi$ is:

$$(\Phi)_i = [\varphi(x_i) - \mu]$$

\paragraph{} The covariance matrix in kernel-space can now be expressed as:

$$S = \Phi\Phi^t$$

\paragraph{} The next step would be to compute the eigenvectors of $S$.
However, since its dimensions could potentially be $\infty \times \infty$, it
is desirable to avoid such a computation. We can quickly see that the centered
Gramm matrix can be expressed as $G_\mu = \Phi^t\Phi$ and prove that
the eigenvectors of $\Phi\Phi^t$ can be expressed in term of the eigenvectors
$v_i$ of $\Phi^t\Phi$:

\begin{align}\label{eq:grammcovar}
\Phi^t\Phi v_i &= \beta v_i \\
(\Phi\Phi^t)\Phi v_i &= \beta (\Phi v_i)
\end{align}

\paragraph{} Fortunately, $G_\mu$ is of dimension $N \times N$, and we can
compute its eigenvectors because the dataset $X$ is finite. The expression of
$G_\mu$ is\footnote{Remember that the dot product is a bilinear operator.}:

\begin{align*}
(G_\mu)_{i, j} &= \left<\varphi(x_i) - \mu, \varphi(x_j) - \mu\right>\\
               &= \left<\varphi(x_i), \varphi(x_j)\right> -
                  \left<\varphi(x_i), \mu\right> -
                  \left<\mu, \varphi(x_j)\right> +
                  \left<\mu, \mu\right>\\
               &= \left<\varphi(x_i), \varphi(x_j)\right> -
                  \left<\varphi(x_i), \frac{1}{N}\sum_{n = 1}^N\varphi(x_n)\right> -
                  \left<\frac{1}{N}\sum_{n = 1}^N\varphi(x_n), \varphi(x_j)\right> +
                  \left<\frac{1}{N}\sum_{n = 1}^N\varphi(x_n), \frac{1}{N}\sum_{l = 1}^N\varphi(x_l)\right>\\
               &= \left<\varphi(x_i), \varphi(x_j)\right> -
                  \frac{1}{N}\sum_{n = 1}^N\left<\varphi(x_i), \varphi(x_n)\right> -
                  \frac{1}{N}\sum_{n = 1}^N\left<\varphi(x_n), \varphi(x_j)\right> +
                  \frac{1}{N^2}\sum_{n = 1}^N\sum_{l = 1}^N\left<\varphi(x_n), \varphi(x_l)\right>\\
               &= k(x_i, x_j) - \underbrace{\frac{1}{N}\sum_{n = 1}^N k(x_i, x_n)}_{\text{mean of column of G}} - \underbrace{\frac{1}{N}\sum_{n = 1}^N k(x_n, x_j)}_{\text{mean of line of G}} + \underbrace{\frac{1}{N^2}\sum_{n = 1}^{N}\sum_{l = 1}^{N} k(x_n, x_l)}_{\text{mean of G}}
\end{align*}
% &= k\left(\varphi(x_i) - \mu, \varphi(x_j) - \mu\right) \\


\paragraph{} We know from equation (\ref{eq:grammcovar}) that if $(\beta, v)$
is a couple (eigenvalue, eigenvector) of $G_\mu$, then $(\beta, \Phi v)$ is its
counterpart for covariance matrix $S$. Moreover, eigenvectors must be
normalized \emph{in kernel-space} which translates in the following:

\begin{align*}
&\| \Phi v \|^2 = 1 \\
\Rightarrow\quad &(\Phi v_i)^t(\Phi v) = 1 \\
\Rightarrow\quad &v^t\Phi^t\Phi v = 1 \\
\Rightarrow\quad &v^t G_\mu v = 1
\end{align*}

\paragraph{} We define the scalar $\alpha$ as $\alpha = v^t G_\mu v$. Since $v$
is an eigenvector of $G_\mu$ the eigenvector $v' = \frac{v}{\sqrt{\alpha}}$ is
normalized in kernel-space and $\Phi v'$ is a normalized eigenvector of $S$.


\paragraph{} The last step is to project the data onto the new space defined by
the eigenvectors.  This is done by projecting $\varphi(x)$ onto $\Phi v'$ which
transposes in computing the following dot product:

\begin{align*}
\langle\varphi(x) - \mu, \Phi v'\rangle &= \varphi(x)^t\Phi v' - \mu^t\Phi v' \\
&= (\varphi(x) - \mu)^t\Phi v'
\end{align*}
