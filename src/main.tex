\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\title{Dimensionality Reduction}
\author{Benjamin Roux \\ RÃ©mi Berson}
\date{}


\begin{document}
\maketitle

\section*{Notations}

\begin{itemize}
  \item Let $K$ be the number of classes.
  \item Let $k \in \{1, \ldots, K\}$ be the class index. 
  \item Let $N_c$ be the number of vectors in class $c$.
  \item Let $X = \{x^{c_1}_1, \ldots, x^{c_N}_N\}$ be the set with a cardinality of $N$ containing vectors of dimension $D$ each belonging to a class $c_i$.
  \item Let $\mu$ be the center of gravity of the whole set $X$.
  \item Let $\mu_k$ be the center of gravity of the vectors from class $k$.
  \item The kernel function $k$ is defined by:
  	\begin{align*}
   		k : \left(\mathbb{R}^D\right)^2 &\to \mathbb{R}\\
   		(x_1, x_2) &\mapsto s 
   	\end{align*}
  \item Let $G$ be the Gramm matrix defined by $G_{i,j} = k(x_i, x_j)$.
  \item Let $S_w$ and $S_b$ respectively be the withtin class covariance and the between class covariance. 
\end{itemize}



\section*{Standard PCA}

\paragraph{}
The goal of the Principal Component Analysis (PCA) is to linearly decorelate the data in $X$ by projecting each $x_i$, regardless of its class on the most representative hyperplane.

\paragraph{}
We first compute the covariance matrix for the whole data set:

$$ S = \sum_{i = 1}^N (x_i - \mu)(x_i - \mu)^t $$

\paragraph{}
We notice that, $S$ has dimensions $N \times N$ because it represents the covariance of the data in \textbf{feature space}.

\paragraph{}
Next, we compute the eigen values and vectors of matrix $S$ and sort them in descent order based on the eigen values.

\paragraph{}
The first $M \in \{1, \ldots, N\}$ eigen vectors, also called factorial axes, define the hyperplane on which to project the original data.

\paragraph{}
The data $X$ is projected onto the hyperplane using the projection matrix of size $M \times N$:
$$ P = \left( \begin{array}{ccc}
 &  &  \\
V_1 & \cdots & V_M \\
 &  &  \end{array} \right) $$ 

\paragraph{}
For an other given set $X'$ containing vectors taken from the same distribution as the vectors of $X$, we can decorelate $X'$ using P with:

$$ X'_d = PX'$$

\section*{Standard LDA}


\paragraph{}

The Linear Discriminant Analysis (LDA) also called Fisher's disciminant analysis (FDA) is a classification method that projects data living in $D$ dimensions onto a hyperplane of dimension $D'$ with $D' < D$ according to Fisher's criterion. 

\paragraph{}

A projection that satisfies the Fisher's criterion is a projection that maximizes the distance between the means of the two classes while minimizing the covariance within each class.

\paragraph{}

As opposed to the PCA, the LDA is a supervised method in the sense that it requires forward knowledge of the class for each data point.

We need to know the labels of the data to proceed to the analysis. LDA first appeared as a method able to work with data divided in two classes, but it is easy to generalize to multi-class configuration.

\paragraph{}
We start by expressing the inter ($S_b$) and intra ($S_w$) class variance as follow:

\begin{equation*}
S_b = \frac{1}{N}\sum_{i = 1}^N N_k(m_k - m)(m_k - m)^t
\end{equation*}

Where $m$ and $m_k$ are respectively the global mean and per-class mean of the data defined as:

\begin{align*}
m &= \frac{1}{N}\sum_{i = 1}^{N} x_i \\
m_k &= \frac{1}{N_k}\sum_{i = 1}^{N_k} x_i^k
\end{align*}

\begin{equation*}
S_w = \sum_{k = 1}^{K}\frac{1}{N_k}\sum_{i = 1}^{N_k} (x_i - m_k)(x_i - m_k)^t
\end{equation*}

\paragraph{}
The idea to find the best hyperplan $\bar{w}$ is to \emph{maximize} the quotient:

\begin{equation*}
\boxed{\operatorname*{arg\,max}_w \frac{w^tS_bw}{w^tS_ww}}
\end{equation*}

\paragraph{}
To this end, it is sufficient to compute the $d$ biggest eigenvalues of:

\begin{equation*}
S_w^{-1}S_b
\end{equation*}

\paragraph{}
And then to project the data $x$ onto the hyperplan:

\begin{equation*}
x_p = \bar{w}x
\end{equation*}


\section*{Kernel trick}

\paragraph{}
The two previous methods assume that our data are linearly separable in feature space. But what if this is not the case ? It is possible to bypass this limitation by working in a high-dimensional space called \emph{kernel-space}, thanks to a mapping function $\varphi$. The idea is then to use $\varphi(x)$ where we previously used $x$.

\paragraph{}
Are we done ? Not yet, since the computation of $\varphi(x)$ may be intractable because the \emph{kernel-space} is allowed to have a dimension equal to infinity. To avoid this difficulty, we use what we call the \emph{kernel trick}. Instead of computing $\varphi(x)$, we will use a \emph{kernel function} $k(x, y) = \varphi(x)^t\varphi(y)$ that computes the dot product of $x$ and $y$ in \emph{kernel-space} so that we never have to compute the result of $\varphi$ directly. To use the trick, we must adapt our formules to replace every occurence of $\varphi(x)$ by a dot product with another vector. Let's see what we get with PCA and LDA.


\section*{Kernel PCA}

\paragraph{}
As for the standard PCA, we must center the vectors of the data set by subtracting the global mean to each one. We must first compute the mean in the kernel-space:

$$\bar{\varphi} = \frac{1}{N}\sum_{i = 1}^{N} \varphi(x_i)$$

This formule is intractable. Fortunately, since what we want to compute is $\varphi(x_i) - \bar{\varphi}$ for every $x_i$, it is sufficient to be able to compute a \emph{distance} in kernel-space. We achieve this as follow:

$$D(\varphi(x), \varphi(y)) = \|\varphi(x) - \varphi(y)\|^2 = k(x, x) - 2k(x, y) + k(y, y)$$

\paragraph{}
By replacing $\varphi(y)$ by $\bar{\varphi}$ in the previous equation, we obtain:

$$D(\varphi(x), \bar{\varphi}) = k(y, y) - \underbrace{\frac{2}{N}\sum_{i = 1}^N k(y, x_i)}_{\text{mean of a line of G}} + \underbrace{\frac{1}{N^2}\sum_i\sum_j k(x_i, x_j)}_{\text{mean of G}}$$

\paragraph{}
Let $\Phi$ be the data set centered in kernel-space:

$$\Phi_i = [\varphi(x_i) - \bar{\varphi}] = [D(\varphi(x_i), \bar{\varphi})]$$

\paragraph{}
The covariance matrix in kernel-space can now be expressed as:

$$S = \Phi\Phi^t$$

\paragraph{}
The next step is to compute the eigenvectors of $S$, but since its dimension could be $\infty \times \infty$, it is impossible to do. But we can quickly see that the centered Gramm matrix $\bar{K} = \Phi^t\Phi$, and we can prove that $\Phi^t\Phi$ and $\Phi\Phi^t$ have the same eigenvectors:

\begin{align}\label{eq:grammcovar}
\Phi^t\Phi v &= \lambda v \\
(\Phi\Phi^t)\Phi v &= \lambda (\Phi v)
\end{align}

\paragraph{}
Fortunately, $\bar{K}$ is of dimension $N \times N$, and we can compute its eigenvectors. The expression of $\bar{K}$ is:

\begin{align*}
\bar{K}_{i, j} &= k(\varphi(x_i) - \bar{\varphi}, \varphi(x_j) - \bar{\varphi}) \\
&= k(x_i, x_j)
	- \underbrace{\frac{1}{N}\sum_{k = 1}^N k(x_k, x_j)}_{\text{mean of column of G}}
    - \underbrace{\frac{1}{N}\sum_{k = 1}^N k(x_i, x_k)}_{\text{mean of line of G}}
    + \underbrace{\frac{1}{N^2}\sum_k\sum_l k(x_k, x_l)}_{\text{mean of G}}
\end{align*}

\paragraph{}
We know from equation (\ref{eq:grammcovar}) that if $(\lambda, v)$ is a couple (eigen value, eigen vector) of $\bar{K}$, then $(\lambda, \Phi v)$ is its counterpart for covariance matrix $S$. Moreover, eigenvectors must be normalized \emph{in kernel-space}, that is:

\begin{align*}
&\| \Phi v \|^2 = 1 \\
\Rightarrow\quad &(\Phi v)^t(\Phi v) = 1 \\
\Rightarrow\quad &v^t\Phi^t\Phi v = 1 \\
\Rightarrow\quad &v^t \bar{K} v = 1
\end{align*}

\paragraph{}
Let then $\alpha = v^t \bar{K} v$, if $v$ is an eigenvector of $\bar{K}$, then the eigenvector $v' = \frac{v}{\sqrt{\alpha}}$ is normalized in kernel-space and $\Phi v'$ is a normalized eigenvector of $S$.


\paragraph{}
The last step is to project our data onto the new space defined by the eigenvectors, that is, project $\varphi(x)$ onto $\Phi v'$. This is done by computing the dot product:

\begin{align*}
<\varphi(x) - \bar{\varphi}, \Phi v'> &= \varphi(x)^t\Phi v' - \bar{\varphi}^t\Phi v' \\
&= (\varphi(x) - \bar{\varphi})^t\Phi v' \\
&= [\bar{K}(x, x_1), \ldots, \bar{K}(x, x_n)] v'
\end{align*}


\section*{Kernel LDA}

\paragraph{}
As for the Kernel PCA, we must adapt the equations of LDA to work with the kernel trick.
% TODO, Benji a toi de jouer ! Tu as pris plus de notes que moi sur les developpements

\end{document}