
\paragraph{}
The goal of the Principal Component Analysis (PCA) is to \emph{linearly} decorelate the data in $X$ by projecting each $x_i$, regardless of its class on hyperplane most representative of the variance of the data-set.

\paragraph{}
We first compute the covariance matrix for the whole data set (note that each vector is centered by subtracting the global mean $\mu$):

$$ S = \sum_{i = 1}^N (x_i - \mu)(x_i - \mu)^t $$

\paragraph{}
We notice that, $S$ has dimensions $N \times N$ because it represents the covariance of the data in \emph{feature space}.

\paragraph{}
Next, we compute the eigen values and vectors of matrix $S$ and sort them in descent order based on their respective eigen values.

\paragraph{}
The first $M \in \{1, \ldots, N\}$ eigen vectors, also called factorial axes, define the hyperplane on which to project the original data.

\paragraph{}
The data $X$ is projected onto the hyperplane using the projection matrix of size $D \times M$:
$$ P = \left( \begin{array}{ccc}
 &  &  \\
V_1 & \cdots & V_M \\
 &  &  \end{array} \right) $$ 

\paragraph{}
For an other given set $X'$ containing vectors taken from the same distribution as the vectors of $X$, we can decorelate $X'$ using P with:

$$ X'_p = PX'$$
