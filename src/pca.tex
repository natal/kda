
\paragraph{}
The goal of the Principal Component Analysis (PCA) is to linearly decorelate the data in $X$ by projecting each $x_i$, regardless of its class on the most representative hyperplane.

\paragraph{}
We first compute the covariance matrix for the whole data set:

$$ S = \sum_{i = 1}^N (x_i - \mu)(x_i - \mu)^t $$

\paragraph{}
We notice that, $S$ has dimensions $N \times N$ because it represents the covariance of the data in \textbf{feature space}.

\paragraph{}
Next, we compute the eigen values and vectors of matrix $S$ and sort them in descent order based on the eigen values.

\paragraph{}
The first $M \in \{1, \ldots, N\}$ eigen vectors, also called factorial axes, define the hyperplane on which to project the original data.

\paragraph{}
The data $X$ is projected onto the hyperplane using the projection matrix of size $M \times N$:
$$ P = \left( \begin{array}{ccc}
 &  &  \\
V_1 & \cdots & V_M \\
 &  &  \end{array} \right) $$ 

\paragraph{}
For an other given set $X'$ containing vectors taken from the same distribution as the vectors of $X$, we can decorelate $X'$ using P with:

$$ X'_d = PX'$$
